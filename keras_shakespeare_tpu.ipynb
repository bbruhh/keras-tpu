{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_shakespeare_tpu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IOvxQVAKNRpi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras Shakespeare TPU"
      ]
    },
    {
      "metadata": {
        "id": "oc2P8Xt7UDFV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2018 The TensorFlow Authors.  \n",
        "Licensed under the Apache License, Version 2.0  \n",
        "You may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License.  "
      ]
    },
    {
      "metadata": {
        "id": "Yb3XqI64M_Jr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pprint\n",
        "import six\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding, LSTM, TimeDistributed, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwno6SgMNfsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check our TPU"
      ]
    },
    {
      "metadata": {
        "id": "5oDbGR9TNkoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "976e3ec9-8039-4a66-ea48-67d8863f762d"
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  device_name = os.environ['COLAB_TPU_ADDR']\n",
        "  print('Found TPU at: {}'.format('grpc://' + device_name))\n",
        "  \n",
        "  with tf.Session('grpc://' + device_name) as session:\n",
        "    pprint.pprint(session.list_devices())\n",
        "\n",
        "except KeyError:\n",
        "  print('TPU not found, please check your runtime type.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at: grpc://10.114.90.138:8470\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 14165534068721807808),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11175135258560929492),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 126239805025064459),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8846436820245824243),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16461477481542244409),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 503497361210230062),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1958969537253387039),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7401261502090926438),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 51587987143692260),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2096091053877854841),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3804966037924571987),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9606362930426045804)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cA_GPvTNNzUY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download data"
      ]
    },
    {
      "metadata": {
        "id": "l0YKd5UrNrIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "15c83b6d-d6d5-4ab9-f4f1-0f65ee1398f1"
      },
      "cell_type": "code",
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-14 06:32:43--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5852404 (5.6M) [text/plain]\n",
            "Saving to: ‘/content/shakespeare.txt’\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.58M  4.09MB/s    in 1.4s    \n",
            "\n",
            "2018-10-14 06:32:44 (4.09 MB/s) - ‘/content/shakespeare.txt’ saved [5852404/5852404]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "omygdkYAN8GU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare data generator"
      ]
    },
    {
      "metadata": {
        "id": "dBGmVPbIOUWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "9871468b-76c6-4c62-9cc9-a2fa28da78c8"
      },
      "cell_type": "code",
      "source": [
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def transform(txt, pad_to=None):\n",
        "  # drop any non-ascii characters\n",
        "  output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "  if pad_to is not None:\n",
        "    output = output[:pad_to]\n",
        "    output = np.concatenate([\n",
        "        np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
        "        output,\n",
        "    ])\n",
        "  return output\n",
        "\n",
        "def training_generator(seq_len=100, batch_size=1024):\n",
        "  \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
        "  with tf.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
        "    txt = f.read()\n",
        "\n",
        "  tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
        "  source = transform(txt)\n",
        "  while True:\n",
        "    offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
        "\n",
        "    # Our model uses sparse crossentropy loss, but Keras requires labels\n",
        "    # to have the same rank as the input logits.  We add an empty final\n",
        "    # dimension to account for this.\n",
        "    yield (\n",
        "        np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
        "        np.expand_dims(\n",
        "            np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
        "            -1),\n",
        "    )\n",
        "\n",
        "six.next(training_generator(seq_len=10, batch_size=1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Input text [5834393] ﻿\r\n",
            "Project Gutenberg’s The Complete Works of Willi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[114, 115,  13,  10,  72,  97, 116, 104,  32, 118]], dtype=int32),\n",
              " array([[[115],\n",
              "         [ 13],\n",
              "         [ 10],\n",
              "         [ 72],\n",
              "         [ 97],\n",
              "         [116],\n",
              "         [104],\n",
              "         [ 32],\n",
              "         [118],\n",
              "         [111]]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "325t9uSHPDjB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build model"
      ]
    },
    {
      "metadata": {
        "id": "m4ho_cGYPHZx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  predicted_char = TimeDistributed(Dense(256, activation='softmax'))(lstm_2)\n",
        "  model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
        "  model.compile(\n",
        "      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AZ9S56PlP1ik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ]
    },
    {
      "metadata": {
        "id": "dQIQqGS4P5yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "8141b9c5-0683-45c3-e7e4-fec73d82c583"
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "training_model = lstm_model(seq_len=100, batch_size=128, stateful=False)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.fit_generator(\n",
        "    training_generator(seq_len=100, batch_size=1024),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10,\n",
        ")\n",
        "\n",
        "tpu_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.114.90.138:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14165534068721807808)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11175135258560929492)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 126239805025064459)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8846436820245824243)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16461477481542244409)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 503497361210230062)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1958969537253387039)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7401261502090926438)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 51587987143692260)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2096091053877854841)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3804966037924571987)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9606362930426045804)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: b'grpc://10.114.90.138:8470'\n",
            "Epoch 1/10\n",
            "INFO:tensorflow:Input text [5834393] ﻿\n",
            "Project Gutenberg’s The Complete Works of Willi\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.int32, name='seed0'), TensorSpec(shape=(128, 100, 1), dtype=tf.float32, name='time_distributed_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for seed\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.88517689704895 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "100/100 [==============================] - 26s 261ms/step - loss: 4.5135 - sparse_categorical_accuracy: 0.1850\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 3.2130 - sparse_categorical_accuracy: 0.2120\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 1.9757 - sparse_categorical_accuracy: 0.4268\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 17s 166ms/step - loss: 1.3947 - sparse_categorical_accuracy: 0.5768\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 1.2606 - sparse_categorical_accuracy: 0.6108\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 1.2013 - sparse_categorical_accuracy: 0.6264\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 17s 168ms/step - loss: 1.1687 - sparse_categorical_accuracy: 0.6351\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 17s 167ms/step - loss: 1.1426 - sparse_categorical_accuracy: 0.6415\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 17s 165ms/step - loss: 1.1262 - sparse_categorical_accuracy: 0.6465\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 1.1153 - sparse_categorical_accuracy: 0.6495\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JGltotzZQDLY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make predictions with the model"
      ]
    },
    {
      "metadata": {
        "id": "G257YDZFQHpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "c825868e-5ec2-4044-b74b-b1bdbd441cd3"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "  prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "  \n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  generated = ''.join([chr(c) for c in p])\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " PR RUCHSENS MOREN THITH sERVANT\r\n",
            "  BERTRAM WYMANMAN enters an Attendant\r\n",
            "\r\n",
            "SCENE VI. Another part of the LORDS\r\n",
            "  SPEED. Well, he strain thrown with what it is both he\r\n",
            "    had but easily begin to show our extremes.\r\n",
            "  IAGO. Will I have fill not do \n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " very enough,\r\n",
            "    For now he mistress it on the deputy with clarge.\r\n",
            "    And with me at that time I breathe cost\r\n",
            "    Shall repent them. York canof not reprehend them all.\r\n",
            "    Now, by the Duke's daughter, if you will.\r\n",
            "  PORTIA. We shall keep your \n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            "           Exit MESSENGER\r\n",
            "  HASTINGS. We know me, the Troy burnet him. All's not lov'd. A children of Mytilene. You may report\r\n",
            "    Their friends out of a poison'd Philippi.\r\n",
            "  BARNARDINE. Sirrahnets at her general, &c. \r\n",
            "  ÏIET JUSTICE. Not one min\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " vict a will I,\r\n",
            "    A maid may I be extremely in blood- thou hast got smells;\r\n",
            "    For yet do I still put to your honour, while we\r\n",
            "    findle upon your glasse is most untuninuance where it mocks\r\n",
            "    I mean'd am a stour cheekmen of yours.\r\n",
            "  GUIDER\n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " I had a smact mistrust\r\n",
            "    Confession. Let't time would make the oppose thou sire a will\r\n",
            "    O'ertrust the gracious liege, which is false rage\r\n",
            "    From Friar John is he to one, that men do\r\n",
            "have made him soundly ere we to cherish yet trust me;\r\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}